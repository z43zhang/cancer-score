# ğŸŒ **Live Demo**: [Click here to try it out](https://cancer-score.streamlit.app/)

![App Preview](https://github.com/z43zhang/cancer-score/blob/main/assets/demo_image.png)

### ğŸ©º A machine learning regression app for predicting cancer severity scores using patient lifestyle, environmental, and clinical data.

---

# ğŸš€ Features

* ğŸ“Š **EDA & Data Cleaning** â€” Explored variable distributions, outliers, feature correlations, and groupwise patterns
* ğŸ”¢ **Multimodal Feature Support** â€” Handled both numerical (e.g., Age, Risk Factors) and categorical (e.g., Gender, Region, Cancer Type) data via preprocessing pipeline
* ğŸ§° **End-to-End ML Pipeline** â€” Built with `ColumnTransformer` for modular imputation, encoding, and scaling
* ğŸ” **Model Exploration** â€” Benchmarked a wide range of regressors: Linear, Ridge, Lasso, ElasticNet, Bayesian Ridge, KNN, CatBoost, LightGBM, and Stacking
* ğŸ”§ **Hyperparameter Tuning** â€” Fine-tuned top models using `GridSearchCV` and `Optuna`, comparing baseline vs optimized performance
* ğŸ“‰ **PCA-Based Dimensionality Analysis** â€” Used PCA to assess feature redundancy and evaluated models under dimensionality constraints
* ğŸ¤– **Final Model: ElasticNet (Tuned)** â€” Chosen for strong generalization, stability across folds, and clinical interpretability
* âš™ï¸ **Live Inference App** â€” Real-time prediction with interactive UI and SHAP waterfall visualization, deployed on Streamlit Cloud
* ğŸ“˜ **Severity Reference Guide** â€” Shows percentile ranking within the dataset and provides interpretation of predicted severity level 

---

# ğŸ“‚ Dataset

- Source: [Global Cancer Patients 2015â€“2024 (Kaggle)](https://www.kaggle.com/datasets/zahidmughal2343/global-cancer-patients-2015-2024/data)
- Records: 50,000 entries from 2015 to 2024
- Features include:
  - Demographics (Age, Gender, Country)
  - Lifestyle factors (Smoking, Alcohol Use, Obesity Level)
  - Environmental exposure (Air Pollution)
  - Medical history (Genetic Risk, Cancer Type)
- Target: `Target_Severity_Score` (continuous)

> ğŸš« Features like `Treatment_Cost_USD`, `Survival_Years`, and `Cancer_Stage` were excluded during training to avoid data leakage.

---

# ğŸ› ï¸ Tech Stack

| **Component**            | **Description**                                                 |
| ------------------------ | --------------------------------------------------------------- |
| `pandas`, `numpy`        | Data manipulation, statistical computations                     |
| `matplotlib`, `seaborn`  | Data visualization for EDA, correlation, PCA, and distributions |
| `scikit-learn`           | Pipelines, preprocessing, PCA, regression models                |
| `joblib`                 | Model and pipeline saving/loading                               |
| `optuna`, `GridSearchCV` | Hyperparameter tuning for both linear and tree-based models     |
| `shap`                   | SHAP-based interpretability for local and global explanations   |
| `catboost`, `lightgbm`   | Advanced tree-based regression models                           |
| `streamlit`              | Frontend interface and app deployment                           |


---

# ğŸ§ª ML Pipeline Breakdown

This project follows a full ML lifecycle: from data exploration and preprocessing to model tuning and deployment. The work is structured into **three modular notebooks** and one deployment script.

## ğŸ““ Notebook Walkthrough

| Notebook | Description |
|----------|-------------|
| [`1_Explore_Data_Analysis.ipynb`](https://github.com/z43zhang/cancer-score/blob/main/notebooks/1_Explore_Data_Analysis.ipynb) | EDA, visualizations, correlation checks |
| [`2_Data_Processing.ipynb`](https://github.com/z43zhang/cancer-score/blob/main/notebooks/2_Data_Processing.ipynb) | Feature engineering, pipeline construction, PCA |
| [`3_Severity_Score_Modeling.ipynb`](https://github.com/z43zhang/cancer-score/blob/main/notebooks/3_Severity_Score_Modeling.ipynb) | Model training, evaluation, hyperparameter tuning |

---

## ğŸ”§ Pipeline & Feature Engineering

- Built a **`ColumnTransformer` pipeline** combining:
  - `StandardScaler` for numerical features
  - `OneHotEncoder` for categorical features
- Applied **PCA** to analyze feature redundancy
  - First 8 components explained ~80% variance

> âœ… All transformations were saved using `joblib` to enable plug-and-play deployment.

---

## ğŸ“‹ Baseline Model Performance

| Model             | RÂ²     | RMSE   | MAE    |
|------------------|--------|--------|--------|
| Bayesian Ridge    | 0.7866 | 0.5509 | 0.4785 |
| Linear Regression | 0.7866 | 0.5510 | 0.4785 |
| Ridge             | 0.7866 | 0.5510 | 0.4785 |
| Stacking Regressor| 0.7826 | 0.5561 | 0.4815 |
| CatBoost          | 0.7815 | 0.5575 | 0.4824 |
| LightGBM          | 0.7801 | 0.5593 | 0.4832 |
| ElasticNet        | 0.7720 | 0.5695 | 0.4880 |
| Lasso             | 0.7550 | 0.5903 | 0.5002 |
| KNN               | 0.6862 | 0.6681 | 0.5525 |

---

## ğŸ”§ Tuned Model Performance

| Model                       | RÂ²     | RMSE   | MAE    | Î”RÂ²    | Î”RMSE   | Î”MAE   |
|----------------------------|--------|--------|--------|--------|---------|--------|
| ElasticNet (Tuned)         | 0.7868 | 0.5507 | 0.4783 | ğŸŸ¢ +0.0148 | ğŸŸ¢ âˆ’0.0188 | ğŸŸ¢ âˆ’0.0097 |
| Lasso (Tuned)              | 0.7868 | 0.5508 | 0.4784 | ğŸŸ¢ +0.0318 | ğŸŸ¢ âˆ’0.0395 | ğŸŸ¢ âˆ’0.0218 |
| Stacking Regressor (Tuned) | 0.7867 | 0.5508 | 0.4783 | ğŸŸ¢ +0.0041 | ğŸŸ¢ âˆ’0.0053 | ğŸŸ¢ âˆ’0.0032 |
| Ridge (Tuned)              | 0.7866 | 0.5509 | 0.4785 | âšª Â±0.0000 | ğŸŸ¢ âˆ’0.0001 | âšª Â±0.0000 |
| CatBoost (Tuned)           | 0.7859 | 0.5519 | 0.4789 | ğŸŸ¢ +0.0044 | ğŸŸ¢ âˆ’0.0056 | ğŸŸ¢ âˆ’0.0035 |
| LightGBM (Tuned)           | 0.7847 | 0.5535 | 0.4798 | ğŸŸ¢ +0.0046 | ğŸŸ¢ âˆ’0.0058 | ğŸŸ¢ âˆ’0.0034 |

---

## ğŸ“‰ PCA-Based Model Evaluation

PCA-transformed inputs (8â€“15 components) were tested to evaluate the impact of dimensionality reduction on model performance.

- PCA did **not significantly improve** predictive accuracy compared to the original feature space.
- Models showed stable performance across 8-15 PCA components, confirming dimensionality reduction can be reduced without performance loss.
- Final models were trained without PCA to maintain full interpretability.

> PCA visualizations and explained variance analysis are available in the [3_Severity_Score_Modeling.ipynb](https://github.com/z43zhang/cancer-score/blob/main/notebooks/3_Severity_Score_Modeling.ipynb).

---

## ğŸ—³ï¸ Final Model Choice

The **ElasticNet (Tuned)** model was selected for its:
- ğŸ§© Balance between bias and variance via combined L1 and L2 regularization
- ğŸ§  Interpretability through feature coefficients â€” aligned with clinical expectations
- ğŸ” Consistently stable results across cross-validation folds
- âš¡ Lightweight and efficient â€” suitable for real-time inference
- ğŸ“ˆ Achieves top-tier performance in RÂ², RMSE, and MAE

> ğŸ§¾ Evaluation metrics (on test set):  
> **RÂ²** = 0.7868 | **RMSE** = 0.5507 | **MAE** = 0.4783

---

## ğŸ–¥ï¸ Deployment (app.py)

- Built with `Streamlit` for real-time predictions
- Loads `elasticnet_tuned.pkl` and `preprocessor.pkl`
- Interactive inputs + top feature breakdown
- Fully hosted via [Streamlit Cloud](https://cancer-score.streamlit.app/)

---

# ğŸ§ª Example Prediction

### ğŸ§¾ Input:

```
Age = 50
Gender = Male
Country = Germany
Year of Diagnosis = 2012
Cancer Type = Liver
Smoking = 2.2
Genetic Risk = 5.8
Air Pollution = 4.5
Obesity Level = 7.2
Alcohol Use = 6.5
```

### ğŸ“‹ Output:

![App Preview](https://github.com/z43zhang/cancer-score/blob/main/assets/example_prediction.png)

---

# ğŸ› ï¸ Installation

```bash
git clone https://github.com/z43zhang/cancer-score-predictor.git
cd cancer-score-predictor
pip install -r requirements.txt
streamlit run app.py
```

---


